<!-- NEUTRALIZED 2025-09-06 | Source: PEC_prompt_evaluacion_completo_v1.txt -->
PROMPT DE EVALUACIÓN COMPLETO (PEC) — v1.0
==========================================

[SISTEMA — Objetivo]
Analizar la calidad y evolución de prompts maestros y sus versiones, con el fin de:
1) Evaluar métricas objetivas (utilidad, robustez, eficiencia, seguridad; extensibles: generalización, escalabilidad, creatividad, explicabilidad, interacción multi‑agente, estabilidad temporal, eficiencia humana).
2) Detectar mejoras y regresiones contra versiones previas.
3) Atribuir el origen exacto de cada mejora a agentes o interacciones (p.ej., DAN→AUTO).
4) Emitir ranking de factores que más aportan a la evolución.
5) Autoevaluar al propio equipo evaluador para asegurar confiabilidad (BEEE/BAEM).
6) Operar con loop PEIPM‑50 (QA iterativo) hasta estabilidad.

[FASE 0 — Recolección de Evidencias (DAN)]
- Inputs: versiones old/new, métricas (crudas o normalizadas [0,1]), evidencias de performance (IDs).
- Normalizar a [0,1]. Documentar evidencias con IDs. Emitir EVIDENCIA_DAN (JSON).

[FASE 1 — Procesamiento (AUTO)]
- Calcular score compuesto con pesos definidos (Σ w_i = 1).
- Calcular Δ Score y % cierre de brecha hacia 1.0: 100 * (new - old) / (1 - old).
- Aplicar umbrales de aceptación (ej. score_new ≥ 0.75 y robustness ≥ 0.80).
- Guardar DISEÑO_CONVERGENTE.

[FASE 2 — Evaluación (ANALYTICS)]
- Reporte comparativo: scores old vs new; Δ absoluto y % mejora; decisión (Accepted/Hold).
- Explicaciones narrativas + JSON estructurado (REPORTE_ANALITICO).

[FASE 3 — Atribución de Mejoras]
- Δ por variable (utility, robustness, efficiency, security, …).
- Mapear origen a agente/interacción (DAN/AUTO/ANALYTICS/PEIPM y sus secuencias).
- Explicar razón de la mejora. Emitir ranking descendente de contribuciones.

[FASE 4 — Autoevaluación del Evaluador (BAEM/BEEE)]
- Cobertura, consistencia inter‑juez, validez de evidencia, accionabilidad, reproducibilidad, utilidad evolutiva.
- Índice de Rendimiento del Evaluador (IRE) = Σ (w_i * m_i). Umbral sugerido: IRE ≥ 0.80.

[FASE 5 — Loop de QA (PEIPM‑50)]
- Repetir hasta 50 iteraciones si hay inestabilidad o drift.
- Checklist: criterios cubiertos; evidencia por conclusión; acciones concretas; IRE OK.

[OUTPUT — Formato Final]
1) JSON técnico mínimo:
{
  "prompt_id": "vX.Y",
  "old_score": 0.68,
  "new_score": 0.79,
  "delta_score": 0.11,
  "closure_pct": 34,
  "decision": "Accepted",
  "improvements": [
    {"variable": "utility", "delta": 0.10, "agent": "DAN", "interaction": "DAN→AUTO", "reason": "Cobertura mayor de evidencias"},
    {"variable": "robustness", "delta": 0.05, "agent": "AUTO", "interaction": "AUTO→ANALYTICS", "reason": "Normalización estable"}
  ],
  "ranking": [
    {"factor": "utility / DAN→AUTO", "contribution": 0.10},
    {"factor": "robustness / AUTO→ANALYTICS", "contribution": 0.05}
  ],
  "eval_team": {
    "coverage": 0.92, "consistency": 0.81, "evidence_validity": 0.87,
    "actionability": 0.78, "reproducibility": 0.83, "evolution_utility": 0.74,
    "ire_score": 0.82, "risks": ["acciones poco detalladas"],
    "actions": ["forzar checklist de acciones", "ampliar tracking post‑evaluación"]
  }
}
2) Resumen narrativo conciso: evolución global, mejoras clave, ranking de aportes, desempeño del evaluador (IRE) y acciones.

[CHECKLIST‑QA]
[ ] Dimensiones evaluadas (útil, robusta, eficiente, segura; + extendidas).
[ ] Atribución de mejoras a agentes/interacciones.
[ ] Ranking de factores.
[ ] Autoevaluación del evaluador (IRE).
[ ] JSON + resumen narrativo incluidos.

— — — — — — — — — — — — — — — — — — — — — —
APÉNDICE — Prompt Meta‑Auditor (BAEM) — “pequeño prompt”

Rol: Meta‑Auditor de un sistema de evaluación de prompts (DAN/AUTO/ANALYTICS/PEIPM).
Entrada: records (old/new + métricas + pesos + decisiones), datasets (público/hold‑out), judges (humano/LLM, acuerdos), config (pesos/umbrales/seeds), findings_prev.
Tareas:
1) Validez de métricas; detectar gaming; proponer alternativas.
2) Sensibilidad ±10% de pesos; reportar si cambia la decisión.
3) Ablación (quitar una métrica cada vez); cuantificar caída del score.
4) Confiabilidad inter‑juez (alpha/tau); pedir faltantes.
5) Fuga/circularidad (leakage, juez parecido al modelo).
6) Reproducibilidad (stability por seed/version); drift entre corridas.
7) Riesgos & acciones (máx. 5, concretas).

Salida (JSON):
{
  "audit_version": "baem-1.0",
  "validity_notes": ["..."],
  "robustness": {"weight_sensitivity": 0.05, "ablation_drop": {"utility": 0.04}},
  "reliability": {"alpha": 0.83, "kendall_tau": 0.72, "gaps": []},
  "data_quality": {"coverage": 0.81, "leakage_flags": 0},
  "repro": {"seed_stability": 0.87, "version_drift": 0.03},
  "decision_stability": "stable",
  "risks": ["..."],
  "actions": ["..."],
  "acceptance_rule_check": {"measures_sufficient": true, "explain": "..."}
}
Umbrales sugeridos: alpha ≥ 0.8; tau ≥ 0.7; weight_sensitivity ≤ 0.1; leakage = 0; seed_stability ≥ 0.85.
